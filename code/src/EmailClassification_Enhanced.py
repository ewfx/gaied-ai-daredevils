#!pip install datasetsfrom transformers import pipelinefrom datasets import load_datasetfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizerfrom datasets import Datasetfrom sklearn.metrics import accuracy_score, classification_reportimport pandas as pdimport numpy as npimport osimport emailimport reimport jsondef fit_training_data_into_model(email_content):      df_trainingdata2 = pd.read_csv("TrainingData2.csv", header=0, encoding='latin-1')  df_trainingdata3 = pd.read_csv("TrainingData3.csv", header=0, encoding='latin-1')  df = pd.concat([df_trainingdata2, df_trainingdata3])  #df = pd.merge(df_trainingdata2, df_trainingdata3, left_index=True, right_index=True)  #print(df.head(40))  df2 = df.Email.to_frame()  ds_train = Dataset.from_pandas(df2)  # Creating training data  df3 = df.PrimaryAsk.to_frame()  ds_primask_test = Dataset.from_pandas(df3)  # Creating primary ask test data  df4 = df.SecondaryAsk.to_frame()  ds_secask_test = Dataset.from_pandas(df4)  # Creating secondary ask test data  # Loading dataset_sheet.csv  df5 = pd.read_csv("dataset_sheet.csv", header=0)  df6 = df5.prompt.to_frame()  ds_train_datasetsheetcsv = Dataset.from_pandas(df6)  # Creating Training data  df7 = df5.label.to_frame()  ds_test_datasetsheetcsv = Dataset.from_pandas(df7)  # Creating Primary Ask test data  df8 = df5.label2.to_frame()  ds_test_datasetsheetcsv2 = Dataset.from_pandas(df8)  # Creating Secondary Ask test data  # Loading train.csv  df7 = pd.read_csv("train.csv", header=0)  df8 = df7.Email.to_frame()  ds_train_traincsv = Dataset.from_pandas(df8)  # Creating train data  df9 = df7.Intent.to_frame()  ds_test_traincsv = Dataset.from_pandas(df9)  # Creating Primary Ask test data  df10 = df7.SecondaryIntent.to_frame()  ds_test_traincsv2 = Dataset.from_pandas(df10)  # Creating Secondary Ask test data  # Use a pipeline as a high-level helper  pipe = pipeline("feature-extraction", model="nicoladecao/msmarco-word2vec256000-distilbert-base-uncased")  # pipe = pipeline("text-classification", model="keshavkmr076/email-intent-classification")  #pipe = pipeline("text-classification", model="alex019/email_sentiment_classifier")  #print(ds)  # Extract data from the 'E' (email) column  emails = ds_train  emails2 = ds_train_datasetsheetcsv  emails3 = ds_train_traincsv  # Extract data from the 'L' (label) column  labels = ds_primask_test  labels2 = ds_secask_test  labels3 = ds_test_datasetsheetcsv  labels4 = ds_test_datasetsheetcsv2  labels5 = ds_test_traincsv  labels6 = ds_test_traincsv2  # Example: Print the first 5 emails and their labels  #for i in range(len(emails3)):  #    print(f"Email: {emails[i]}, Label: {labels5[i]}, Label2: {labels6[i]}")  #for j in range(len(ds2['train'])):  #    print(f"Email: {emails2[i]}, Label: {labels2[i]}")  X_train, X_test, y_train, y_test = train_test_split(emails['Email'] + emails2['prompt'] + emails3['Email'], labels['PrimaryAsk'] + labels3['label'] + labels5['Intent'] , test_size=0.2, random_state=42)  X_train2, X_test2, y_train2, y_test2 = train_test_split(emails['Email'] + emails2['prompt'] + emails3['Email'], labels2['SecondaryAsk'] + labels4['label2'] + labels6['SecondaryIntent'], test_size=0.2, random_state=42)  # Feature extraction using TF-IDF  vectorizer = TfidfVectorizer()  X_train_vec = vectorizer.fit_transform(X_train)  X_test_vec = vectorizer.transform(X_test)  # Feature extraction using TF-IDF for sub request type  vectorizer1 = TfidfVectorizer()  X_train_2_vec = vectorizer1.fit_transform(X_train2)  X_test_2_vec = vectorizer.transform(X_test2)  # Train a Random Forest classifier  rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  rf_classifier.fit(X_train_vec, y_train)  rf_classifier2 = RandomForestClassifier(n_estimators=100, random_state=42)  rf_classifier2.fit(X_train_2_vec, y_train2)  # Make predictions on the test set  y_pred = rf_classifier.predict(X_test_vec)  y_pred2 = rf_classifier2.predict(X_test_2_vec)  # Evaluate the model (e.g., using accuracy, precision, recall, F1-score)  # ... (add your evaluation metrics here)  accuracy = accuracy_score(y_test, y_pred)  print("Accuracy:", accuracy)  #print(classification_report(y_test, y_pred))  # Evaluate the model for sub request type  accuracy2 = accuracy_score(y_test2, y_pred2)  print("Accuracy for sub request type:", accuracy2)  #print(classification_report(y_test2, y_pred2))  new_email = email_content  new_email_vec = vectorizer.transform([new_email])  #new_features = pipe(new_email)[0][0]  predicted_class = rf_classifier.predict(new_email_vec)[0]  predicted_class2 = rf_classifier2.predict(new_email_vec)[0]  print(f"Predicted class for new email: {predicted_class}")  print(f"Predicted secondary class for new email: {predicted_class2}")  return predicted_class + '|' + predicted_class2def extract_email_content(filepath):    """    Extracts email content without greetings and salutations from a file.    Args:        filepath (str): Path to the email file.    Returns:        str: Extracted email content, or None if an error occurs.    """    try:        with open(filepath, 'r', encoding='utf-8') as f:            msg = email.message_from_file(f)        if msg.is_multipart():            for part in msg.walk():                content_type = part.get_content_type()                content_disposition = str(part.get('Content-Disposition'))                if content_type == 'text/plain' and 'attachment' not in content_disposition:                    body = part.get_payload(decode=True).decode()                    break #Take the first plain text part.        else:            body = msg.get_payload(decode=True).decode()        #        # Remove greetings and salutations using regular expressions. This is a basic approach and may need refinement.        body = re.sub(r'^(Dear|Hello|Hi|Greetings|Good morning|Good afternoon|Good evening|Regards|& Regards|Best regards|Sincerely|Thanks|Thank you|Yours sincerely|Yours faithfully),?\s*[\w\s,]*\n?', '', body, flags=re.MULTILINE | re.IGNORECASE)        body = re.sub(r'^(Dear|Hello|Hi|Greetings|Good morning|Good afternoon|Good evening|Regards|& Regards|Best regards|Sincerely|Thanks|Thank you|Yours sincerely|Yours faithfully),?\s*[\w\s,]*$', '', body, flags=re.MULTILINE | re.IGNORECASE) #handles the edge case when the greeting is the only line.        #remove trailing blank lines.        body = body.strip()        return body    except Exception as e:        print(f"Error processing {filepath}: {e}")        return None# Reading attachmentsdef read_email_attachment(email_file_path):   try:  with open(email_file_path, 'rb') as fp:    msg = email.message_from_binary_file(fp)  attachments = []  for part in msg.walk():    if part.get_content_maintype() == 'multipart':      continue    if part.get('Content-Disposition') is None:      continue    filename = part.get_filename()    if filename:      att_data = part.get_payload(decode=True)      attachments.append((filename, att_data))  return attachments except Exception as e:        print(f"Error processing {email_file_path}: {e}")        return None#def process_email_folder(folder_path):    """    Processes all email files in a folder and prints the extracted content.    Args:        folder_path (str): Path to the folder containing email files.    """        subject_line_vec = []    prediction_result_vec = []    attachment_content = ''    for filename in os.listdir(folder_path):        if filename.endswith((".eml", ".txt")):  # Add or remove extensions as needed            filepath = os.path.join(folder_path, filename)            content = extract_email_content(filepath)            attachments = read_email_attachment(filepath)            subject_line = content.splitlines()[0]            print(f"Subject Line: {subject_line}")            if content and (subject_line not in subject_line_vec):                #print(f"Content from {filename}:\n{content}\n{'-'*40}")                prediction_output = fit_training_data_into_model(content.replace("\n", " "))                print(f"Prediction output: {prediction_output}")                prediction_result_vec.append({                "email_text": subject_line,                "request_type": prediction_output.split('|')[0],                "sub_request_type": prediction_output.split('|')[1]                })                #for filename, content in attachments:                #  print(f"Attachment: {filename}")                # You can process the content here, e.g., save it to a file:                #  with open(filename, 'wb') as f:                #    attachment_content = attachment_content + f.write(content)                 # Passing the attachment content to the model                #prediction_output_for_attachment = fit_training_data_into_model(attachment_content)                #print(f"Prediction output for attachment : {prediction_output_for_attachment}")                print("\n")                subject_line_vec.append(subject_line)        with open('/content/Email_Classification_Results.json', 'w') as f:      json.dump(prediction_result_vec, f, indent=4)    print("JSON file created successfully.")                # Example usage:folder_path = "/content/Email_Folder" #replace with your folder path.process_email_folder(folder_path)